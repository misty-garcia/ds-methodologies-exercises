{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pyspark\n",
    "from pydataset import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a spark data frame that contains your favorite programming languages.\n",
    "\n",
    "- The name of the column should be language\n",
    "- View the schema of the dataframe\n",
    "- Output the shape of the dataframe\n",
    "- Show the first 5 records in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs = pd.DataFrame([\"python\", \"r\", \"java\", \"spark\", \"natural\", \"php\"], columns=[\"language\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(langs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- language: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|language|\n",
      "+--------+\n",
      "|  python|\n",
      "|       r|\n",
      "|    java|\n",
      "|   spark|\n",
      "| natural|\n",
      "+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the mpg dataset as a spark dataframe.\n",
    "\n",
    "- Create 1 column of output that contains a message like the one below:\n",
    "    - The 1999 audi a4 has a 4 cylinder engine.\n",
    "    - For each vehicle.\n",
    "\n",
    "- Transform the trans column so that it only contains either manual or auto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg = spark.createDataFrame(data('mpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat, lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+-----+----+---+----------+---+---+---+---+-------+\n",
      "|manufacturer|model|displ|year|cyl|     trans|drv|cty|hwy| fl|  class|\n",
      "+------------+-----+-----+----+---+----------+---+---+---+---+-------+\n",
      "|        audi|   a4|  1.8|1999|  4|  auto(l5)|  f| 18| 29|  p|compact|\n",
      "|        audi|   a4|  1.8|1999|  4|manual(m5)|  f| 21| 29|  p|compact|\n",
      "|        audi|   a4|  2.0|2008|  4|manual(m6)|  f| 20| 31|  p|compact|\n",
      "+------------+-----+-----+----+---+----------+---+---+---+---+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------+\n",
      "|description                                                   |\n",
      "+--------------------------------------------------------------+\n",
      "|The 1999 audi a4 has a 4 cylinder engine.                     |\n",
      "|The 1999 audi a4 has a 4 cylinder engine.                     |\n",
      "|The 2008 audi a4 has a 4 cylinder engine.                     |\n",
      "|The 2008 audi a4 has a 4 cylinder engine.                     |\n",
      "|The 1999 audi a4 has a 6 cylinder engine.                     |\n",
      "|The 1999 audi a4 has a 6 cylinder engine.                     |\n",
      "|The 2008 audi a4 has a 6 cylinder engine.                     |\n",
      "|The 1999 audi a4 quattro has a 4 cylinder engine.             |\n",
      "|The 1999 audi a4 quattro has a 4 cylinder engine.             |\n",
      "|The 2008 audi a4 quattro has a 4 cylinder engine.             |\n",
      "|The 2008 audi a4 quattro has a 4 cylinder engine.             |\n",
      "|The 1999 audi a4 quattro has a 6 cylinder engine.             |\n",
      "|The 1999 audi a4 quattro has a 6 cylinder engine.             |\n",
      "|The 2008 audi a4 quattro has a 6 cylinder engine.             |\n",
      "|The 2008 audi a4 quattro has a 6 cylinder engine.             |\n",
      "|The 1999 audi a6 quattro has a 6 cylinder engine.             |\n",
      "|The 2008 audi a6 quattro has a 6 cylinder engine.             |\n",
      "|The 2008 audi a6 quattro has a 8 cylinder engine.             |\n",
      "|The 2008 chevrolet c1500 suburban 2wd has a 8 cylinder engine.|\n",
      "|The 2008 chevrolet c1500 suburban 2wd has a 8 cylinder engine.|\n",
      "+--------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(concat(lit('The '), mpg.year, lit(' '), mpg.manufacturer, lit(' '), \\\n",
    "                  mpg.model, lit(' has a '), mpg.cyl, lit(' cylinder engine.') ). alias(\"description\")\n",
    "          ).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+-----+----+---+----------+---+---+---+---+-------+\n",
      "|manufacturer|model|displ|year|cyl|     trans|drv|cty|hwy| fl|  class|\n",
      "+------------+-----+-----+----+---+----------+---+---+---+---+-------+\n",
      "|        audi|   a4|  1.8|1999|  4|  auto(l5)|  f| 18| 29|  p|compact|\n",
      "|        audi|   a4|  1.8|1999|  4|manual(m5)|  f| 21| 29|  p|compact|\n",
      "|        audi|   a4|  2.0|2008|  4|manual(m6)|  f| 20| 31|  p|compact|\n",
      "|        audi|   a4|  2.0|2008|  4|  auto(av)|  f| 21| 30|  p|compact|\n",
      "|        audi|   a4|  2.8|1999|  6|  auto(l5)|  f| 16| 26|  p|compact|\n",
      "+------------+-----+-----+----+---+----------+---+---+---+---+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|transmission|\n",
      "+------------+\n",
      "|        auto|\n",
      "|      manual|\n",
      "|      manual|\n",
      "|        auto|\n",
      "|        auto|\n",
      "|      manual|\n",
      "|        auto|\n",
      "|      manual|\n",
      "|        auto|\n",
      "|      manual|\n",
      "|        auto|\n",
      "|        auto|\n",
      "|      manual|\n",
      "|        auto|\n",
      "|      manual|\n",
      "|        auto|\n",
      "|        auto|\n",
      "|        auto|\n",
      "|        auto|\n",
      "|        auto|\n",
      "+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(\n",
    "    regexp_extract(mpg.trans, r'(\\w+)', 1).alias('transmission')\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the tips dataset as a spark dataframe.\n",
    "- What percentage of observations are smokers?\n",
    "- Create a column that contains the tip percentage\n",
    "- Calculate the average tip percentage for each combination of sex and smoker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips = spark.createDataFrame(data('tips'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+------+----+\n",
      "|total_bill| tip|   sex|smoker|day|  time|size|\n",
      "+----------+----+------+------+---+------+----+\n",
      "|     16.99|1.01|Female|    No|Sun|Dinner|   2|\n",
      "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|\n",
      "|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|\n",
      "|     23.68|3.31|  Male|    No|Sun|Dinner|   2|\n",
      "|     24.59|3.61|Female|    No|Sun|Dinner|   4|\n",
      "+----------+----+------+------+---+------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tips.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38114754098360654"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tips.where(tips.smoker == 'Yes').count() / tips.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-------------------+\n",
      "|smoker|count|      (count / 244)|\n",
      "+------+-----+-------------------+\n",
      "|    No|  151| 0.6188524590163934|\n",
      "|   Yes|   93|0.38114754098360654|\n",
      "+------+-----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_n = tips.count()\n",
    "\n",
    "tips.groupBy('smoker').count().select('*', (col('count')/total_n)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|percent|\n",
      "+-------+\n",
      "|   0.06|\n",
      "|   0.16|\n",
      "|   0.17|\n",
      "|   0.14|\n",
      "|   0.15|\n",
      "|   0.19|\n",
      "|   0.23|\n",
      "|   0.12|\n",
      "|   0.13|\n",
      "|   0.22|\n",
      "|   0.17|\n",
      "|   0.14|\n",
      "|    0.1|\n",
      "|   0.16|\n",
      "|    0.2|\n",
      "|   0.18|\n",
      "|   0.16|\n",
      "|   0.23|\n",
      "|   0.21|\n",
      "|   0.16|\n",
      "+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a column that contains the tip percentage\n",
    "tips.select(\n",
    "    round(tips.tip / tips.total_bill,2).alias('percent')\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------------------+\n",
      "|   sex|smoker|        tip_percent|\n",
      "+------+------+-------------------+\n",
      "|  Male|    No| 0.1606687151291298|\n",
      "|  Male|   Yes| 0.1527711752024851|\n",
      "|Female|    No| 0.1569209707691836|\n",
      "|Female|   Yes|0.18215035269941035|\n",
      "+------+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average tip percentage for each combination of sex and smoker\n",
    "tips.groupby('sex', 'smoker').agg(mean(tips.tip / tips.total_bill).alias('tip_percent')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the seattle weather dataset referenced in the lesson to answer the questions below.\n",
    "\n",
    "- Convert the temperatures to farenheight.\n",
    "- Which month has the most rain, on average?\n",
    "- Which year was the windiest?\n",
    "- What is the most frequent type of weather in January?\n",
    "- What is the average high and low tempurature on sunny days in July in 2013 and 2014?\n",
    "- What percentage of days were rainy in q3 of 2015?\n",
    "- For each year, find what percentage of days it rained (had non-zero precipitation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vega_datasets import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = data('seattle_weather').assign(date=lambda df: df.date.astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------+--------+----+-------+\n",
      "|      date|precipitation|temp_max|temp_min|wind|weather|\n",
      "+----------+-------------+--------+--------+----+-------+\n",
      "|2012-01-01|          0.0|    12.8|     5.0| 4.7|drizzle|\n",
      "|2012-01-02|         10.9|    10.6|     2.8| 4.5|   rain|\n",
      "|2012-01-03|          0.8|    11.7|     7.2| 2.3|   rain|\n",
      "|2012-01-04|         20.3|    12.2|     5.6| 4.7|   rain|\n",
      "|2012-01-05|          1.3|     8.9|     2.8| 6.1|   rain|\n",
      "+----------+-------------+--------+--------+----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|temp_max_F|temp_min_F|\n",
      "+----------+----------+\n",
      "|     55.04|      41.0|\n",
      "|     51.08|     37.04|\n",
      "|     53.06|     44.96|\n",
      "|     53.96|     42.08|\n",
      "|     48.02|     37.04|\n",
      "+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert the temperatures to farenheight.\n",
    "df.select(\n",
    "    (df.temp_max * 9/5 + 32).alias('temp_max_F'),\n",
    "    (df.temp_min * 9/5 + 32).alias('temp_min_F')\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import month, year, quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+\n",
      "|month| avg(precipitation)|\n",
      "+-----+-------------------+\n",
      "|    1| 3.7580645161290316|\n",
      "|    2|  3.734513274336283|\n",
      "|    3|  4.888709677419355|\n",
      "|    4|  3.128333333333333|\n",
      "|    5| 1.6733870967741935|\n",
      "|    6| 1.1075000000000002|\n",
      "|    7|0.38870967741935486|\n",
      "|    8| 1.3201612903225806|\n",
      "|    9| 1.9624999999999997|\n",
      "|   10|  4.059677419354839|\n",
      "|   11|  5.354166666666667|\n",
      "|   12|  5.021774193548388|\n",
      "+-----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Which month has the most rain, on average?\n",
    "(\n",
    "    df.withColumn('month', month('date'))\n",
    "    .groupby('month')\n",
    "    .agg(mean('precipitation'))\n",
    "    .sort('month')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, min, max, expr, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|year|         sum(wind)|\n",
      "+----+------------------+\n",
      "|2015|            1153.3|\n",
      "|2013|1100.8000000000002|\n",
      "|2014|1236.5000000000005|\n",
      "|2012|1244.6999999999998|\n",
      "+----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Which year was the windiest?\n",
    "(\n",
    "    df.withColumn('year', year('date'))\n",
    "    .groupby('year')\n",
    "    .agg(sum('wind'))\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----+----+----+----+\n",
      "|weather|drizzle| fog|rain|snow| sun|\n",
      "+-------+-------+----+----+----+----+\n",
      "|    fog|   null|  38|null|null|null|\n",
      "|drizzle|     10|null|null|null|null|\n",
      "|   rain|   null|null|  35|null|null|\n",
      "|    sun|   null|null|null|null|  33|\n",
      "|   snow|   null|null|null|   8|null|\n",
      "+-------+-------+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# What is the most frequent type of weather in January?\n",
    "(\n",
    "    df.filter(month(\"date\") == 1)\n",
    "    .groupBy(\"weather\")\n",
    "    .pivot('weather')\n",
    "    .count()\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------------+\n",
      "|     avg(temp_max)|    avg(temp_min)|\n",
      "+------------------+-----------------+\n",
      "|26.828846153846158|14.18269230769231|\n",
      "+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# What is the average high and low tempurature on sunny days in July in 2013 and 2014?\n",
    "(\n",
    "    df.filter(month(\"date\") == 7)\n",
    "    .filter((year(\"date\") == 2013) | (year(\"date\") == 2014))\n",
    "    .filter(df.weather == 'sun')\n",
    "    .agg(mean(\"temp_max\"), mean(\"temp_min\"))\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|     avg(was_rainy)|\n",
      "+-------------------+\n",
      "|0.18478260869565216|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# What percentage of days were rainy in q3 of 2015?\n",
    "(\n",
    "    df.withColumn('quarter', quarter('date'))\n",
    "    .withColumn('year', year('date'))\n",
    "    .filter(expr(\"year = 2015\"))\n",
    "    .filter(expr('quarter = 3'))\n",
    "    .select(when(col('precipitation') >0, 1).otherwise(0).alias('was_rainy'))\n",
    "    .agg(mean('was_rainy'))\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+\n",
      "|year|     avg(was_rainy)|\n",
      "+----+-------------------+\n",
      "|2015|0.39452054794520547|\n",
      "|2013|0.41643835616438357|\n",
      "|2014|  0.410958904109589|\n",
      "|2012|0.48360655737704916|\n",
      "+----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For each year, find what percentage of days it rained (had non-zero precipitation)\n",
    "(\n",
    "    df.withColumn('year', year('date'))\n",
    "    .withColumn('was_rainy', when(col('precipitation') > 0, 1).otherwise(0))\n",
    "    .groupby('year')\n",
    "    .agg(mean('was_rainy'))            \n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+\n",
      "|year|     avg(was_rainy)|\n",
      "+----+-------------------+\n",
      "|2015|0.39452054794520547|\n",
      "|2013|0.41643835616438357|\n",
      "|2014|  0.410958904109589|\n",
      "|2012|0.48360655737704916|\n",
      "+----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# with sql\n",
    "# For each year, find what percentage of days it rained (had non-zero precipitation)\n",
    "spark.sql('''\n",
    "SELECT year, AVG(was_rainy)\n",
    "FROM (\n",
    "    SELECT *, year(date) as year,\n",
    "    CASE WHEN precipitation > 0 THEN 1 ELSE 0 END as was_rainy\n",
    "    from df\n",
    ") as a\n",
    "GROUP BY year\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
